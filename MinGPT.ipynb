{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Full Adder - minGPT"
      ],
      "metadata": {
        "id": "TuV-O-nD-kV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone minGPT repository\n",
        "!git clone https://github.com/karpathy/minGPT.git\n",
        "%cd /content/minGPT\n",
        "\n",
        "# Install minGPT library\n",
        "!pip install -e .\n",
        "\n",
        "# Download Tiny Shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "id": "4mhvAXZrsY6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from mingpt.utils import set_seed\n",
        "set_seed(3407)\n",
        "import pickle"
      ],
      "metadata": {
        "id": "PXzg2C9S_nnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SortDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for the Sort problem. E.g. for problem length 6:\n",
        "    Input: 0 0 2 1 0 1 -> Output: 0 0 0 1 1 2\n",
        "    Which will feed into the transformer concatenated as:\n",
        "    input:  0 0 2 1 0 1 0 0 0 1 1\n",
        "    output: I I I I I 0 0 0 1 1 2\n",
        "    where I is \"ignore\", as the transformer is reading the input sequence\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, split, length=6, num_digits=3):\n",
        "        assert split in {'train', 'test'}\n",
        "        self.split = split\n",
        "        self.length = length\n",
        "        self.num_digits = num_digits\n",
        "\n",
        "    def __len__(self):\n",
        "        return 10000 # ...\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return self.num_digits\n",
        "\n",
        "    def get_block_size(self):\n",
        "        # the length of the sequence that will feed into transformer,\n",
        "        # containing concatenated input and the output, but -1 because\n",
        "        # the transformer starts making predictions at the last input element\n",
        "        return self.length * 2 - 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # use rejection sampling to generate an input example from the desired split\n",
        "        while True:\n",
        "            # generate some random integers\n",
        "            inp = torch.randint(self.num_digits, size=(self.length,), dtype=torch.long)\n",
        "            # half of the time let's try to boost the number of examples that\n",
        "            # have a large number of repeats, as this is what the model seems to struggle\n",
        "            # with later in training, and they are kind of rate\n",
        "            if torch.rand(1).item() < 0.5:\n",
        "                if inp.unique().nelement() > self.length // 2:\n",
        "                    # too many unqiue digits, re-sample\n",
        "                    continue\n",
        "            # figure out if this generated example is train or test based on its hash\n",
        "            h = hash(pickle.dumps(inp.tolist()))\n",
        "            inp_split = 'test' if h % 4 == 0 else 'train' # designate 25% of examples as test\n",
        "            if inp_split == self.split:\n",
        "                break # ok\n",
        "\n",
        "        # solve the task: i.e. sort\n",
        "        sol = torch.sort(inp)[0]\n",
        "\n",
        "        # concatenate the problem specification and the solution\n",
        "        cat = torch.cat((inp, sol), dim=0)\n",
        "\n",
        "        # the inputs to the transformer will be the offset sequence\n",
        "        x = cat[:-1].clone()\n",
        "        y = cat[1:].clone()\n",
        "        # we only want to predict at output locations, mask out the loss at the input locations\n",
        "        y[:self.length-1] = -1\n",
        "        return x, y\n"
      ],
      "metadata": {
        "id": "HkI1P4qp-kES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print an example instance of the dataset\n",
        "train_dataset = SortDataset('train')\n",
        "test_dataset = SortDataset('test')\n",
        "x, y = train_dataset[0]\n",
        "for a, b in zip(x,y):\n",
        "    print(int(a),int(b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42zldMSu-yMh",
        "outputId": "5efa26b3-9596-4f30-d632-56f2e9a729a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 -1\n",
            "2 -1\n",
            "0 -1\n",
            "0 -1\n",
            "1 -1\n",
            "2 0\n",
            "0 0\n",
            "0 1\n",
            "1 2\n",
            "2 2\n",
            "2 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a GPT instance\n",
        "from mingpt.model import GPT\n",
        "\n",
        "model_config = GPT.get_default_config()\n",
        "model_config.model_type = 'gpt-nano'\n",
        "model_config.vocab_size = train_dataset.get_vocab_size()\n",
        "model_config.block_size = train_dataset.get_block_size()\n",
        "model = GPT(model_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHVmRU8y_yuw",
        "outputId": "154f3798-929d-4078-e67b-e91a3808d472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 0.09M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a Trainer object\n",
        "from mingpt.trainer import Trainer\n",
        "\n",
        "train_config = Trainer.get_default_config()\n",
        "train_config.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\n",
        "train_config.max_iters = 2000\n",
        "train_config.num_workers = 0\n",
        "trainer = Trainer(train_config, model, train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jJN71Yk_yr5",
        "outputId": "139d64eb-baec-42ca-aac8-34e7c4d43bb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running on device cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_end_callback(trainer):\n",
        "    if trainer.iter_num % 100 == 0:\n",
        "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
        "trainer.set_callback('on_batch_end', batch_end_callback)\n",
        "\n",
        "trainer.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erWt8z7K_ypS",
        "outputId": "f04d72e1-54c8-46e8-b4b4-56f335400ed8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter_dt 0.00ms; iter 0: train loss 1.07424\n",
            "iter_dt 89.01ms; iter 100: train loss 0.18659\n",
            "iter_dt 40.16ms; iter 200: train loss 0.07534\n",
            "iter_dt 41.67ms; iter 300: train loss 0.05724\n",
            "iter_dt 45.23ms; iter 400: train loss 0.10420\n",
            "iter_dt 41.38ms; iter 500: train loss 0.01440\n",
            "iter_dt 40.05ms; iter 600: train loss 0.02197\n",
            "iter_dt 41.39ms; iter 700: train loss 0.00708\n",
            "iter_dt 39.01ms; iter 800: train loss 0.01678\n",
            "iter_dt 39.91ms; iter 900: train loss 0.00429\n",
            "iter_dt 40.21ms; iter 1000: train loss 0.00942\n",
            "iter_dt 41.50ms; iter 1100: train loss 0.02191\n",
            "iter_dt 46.88ms; iter 1200: train loss 0.00303\n",
            "iter_dt 42.90ms; iter 1300: train loss 0.01519\n",
            "iter_dt 53.47ms; iter 1400: train loss 0.00552\n",
            "iter_dt 46.35ms; iter 1500: train loss 0.00074\n",
            "iter_dt 44.74ms; iter 1600: train loss 0.00125\n",
            "iter_dt 47.38ms; iter 1700: train loss 0.00329\n",
            "iter_dt 42.94ms; iter 1800: train loss 0.00181\n",
            "iter_dt 51.37ms; iter 1900: train loss 0.00429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now let's perform some evaluation\n",
        "model.eval();"
      ],
      "metadata": {
        "id": "nTNTZKFM_ymX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_split(trainer, split, max_batches):\n",
        "    dataset = {'train':train_dataset, 'test':test_dataset}[split]\n",
        "    n = train_dataset.length # naugy direct access shrug\n",
        "    results = []\n",
        "    mistakes_printed_already = 0\n",
        "    loader = DataLoader(dataset, batch_size=100, num_workers=0, drop_last=False)\n",
        "    for b, (x, y) in enumerate(loader):\n",
        "        x = x.to(trainer.device)\n",
        "        y = y.to(trainer.device)\n",
        "        # isolate the input pattern alone\n",
        "        inp = x[:, :n]\n",
        "        sol = y[:, -n:]\n",
        "        # let the model sample the rest of the sequence\n",
        "        cat = model.generate(inp, n, do_sample=False) # using greedy argmax, not sampling\n",
        "        sol_candidate = cat[:, n:] # isolate the filled in sequence\n",
        "        # compare the predicted sequence to the true sequence\n",
        "        correct = (sol == sol_candidate).all(1).cpu() # Software 1.0 vs. Software 2.0 fight RIGHT on this line haha\n",
        "        for i in range(x.size(0)):\n",
        "            results.append(int(correct[i]))\n",
        "            if not correct[i] and mistakes_printed_already < 3: # only print up to 5 mistakes to get a sense\n",
        "                mistakes_printed_already += 1\n",
        "                print(\"GPT claims that %s sorted is %s but gt is %s\" % (inp[i].tolist(), sol_candidate[i].tolist(), sol[i].tolist()))\n",
        "        if max_batches is not None and b+1 >= max_batches:\n",
        "            break\n",
        "    rt = torch.tensor(results, dtype=torch.float)\n",
        "    print(\"%s final score: %d/%d = %.2f%% correct\" % (split, rt.sum(), len(results), 100*rt.mean()))\n",
        "    return rt.sum()\n",
        "\n",
        "# run a lot of examples from both train and test through the model and verify the output correctness\n",
        "with torch.no_grad():\n",
        "    train_score = eval_split(trainer, 'train', max_batches=50)\n",
        "    test_score  = eval_split(trainer, 'test',  max_batches=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inVqNKvf_yj_",
        "outputId": "a31cd3b5-b660-46ed-cd32-f24413d7f84b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train final score: 5000/5000 = 100.00% correct\n",
            "test final score: 5000/5000 = 100.00% correct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's run a random given sequence through the model as well\n",
        "n = train_dataset.length # naugy direct access shrug\n",
        "inp = torch.tensor([[0, 0, 2, 1, 0, 1]], dtype=torch.long).to(trainer.device)\n",
        "assert inp[0].nelement() == n\n",
        "with torch.no_grad():\n",
        "    cat = model.generate(inp, n, do_sample=False)\n",
        "sol = torch.sort(inp[0])[0]\n",
        "sol_candidate = cat[:, n:]\n",
        "print('input sequence  :', inp.tolist())\n",
        "print('predicted sorted:', sol_candidate.tolist())\n",
        "print('gt sort         :', sol.tolist())\n",
        "print('matches         :', bool((sol == sol_candidate).all()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAkCFLkI_yg_",
        "outputId": "0a49abf8-8d02-47b0-ad5d-7362aaa4d5a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input sequence  : [[0, 0, 2, 1, 0, 1]]\n",
            "predicted sorted: [[0, 0, 0, 1, 1, 2]]\n",
            "gt sort         : [0, 0, 0, 1, 1, 2]\n",
            "matches         : True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JUnCQI7eOdP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shakespeare - minGPT"
      ],
      "metadata": {
        "id": "U64-Wf0gPJmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone minGPT repository\n",
        "!git clone https://github.com/karpathy/minGPT.git\n",
        "%cd /content/minGPT\n",
        "\n",
        "# Install minGPT library\n",
        "!pip install -e .\n",
        "\n",
        "# Download Tiny Shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFsvNXtlWixy",
        "outputId": "f448c8e1-91a0-4f54-da27-b6cbf2b7f338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'minGPT' already exists and is not an empty directory.\n",
            "/content/minGPT\n",
            "Obtaining file:///content/minGPT\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from minGPT==0.0.1) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->minGPT==0.0.1) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->minGPT==0.0.1) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->minGPT==0.0.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->minGPT==0.0.1) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->minGPT==0.0.1) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->minGPT==0.0.1) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->minGPT==0.0.1) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->minGPT==0.0.1) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->minGPT==0.0.1) (1.3.0)\n",
            "Installing collected packages: minGPT\n",
            "  Attempting uninstall: minGPT\n",
            "    Found existing installation: minGPT 0.0.1\n",
            "    Uninstalling minGPT-0.0.1:\n",
            "      Successfully uninstalled minGPT-0.0.1\n",
            "  Running setup.py develop for minGPT\n",
            "Successfully installed minGPT-0.0.1\n",
            "--2023-12-04 01:49:04--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.3’\n",
            "\n",
            "input.txt.3         100%[===================>]   1.06M  4.94MB/s    in 0.2s    \n",
            "\n",
            "2023-12-04 01:49:04 (4.94 MB/s) - ‘input.txt.3’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "ABDjSDTwkcln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLa--NpIkcjH",
        "outputId": "692c8677-3645-44aa-88a2-9d94f2694af4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2S0lAyxPkcfq",
        "outputId": "fdf6f491-0301-4826-c093-a5dda061f6ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F18pHlvzkcZ6",
        "outputId": "a0a46cdc-3ac0-4b5c-81da-d871032c6154"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from mingpt.model import GPT\n",
        "from mingpt.trainer import Trainer\n",
        "from mingpt.utils import set_seed, setup_logging, CfgNode as CN\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Emits batches of characters\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def get_default_config():\n",
        "        C = CN()\n",
        "        C.block_size = 128\n",
        "        return C\n",
        "\n",
        "    def __init__(self, config, data):\n",
        "        self.config = config\n",
        "\n",
        "        chars = sorted(list(set(data)))\n",
        "        data_size, vocab_size = len(data), len(chars)\n",
        "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "\n",
        "        self.stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "        self.itos = {i: ch for i, ch in enumerate(chars)}\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = data\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return self.vocab_size\n",
        "\n",
        "    def get_block_size(self):\n",
        "        return self.config.block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.config.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab a chunk of (block_size + 1) characters from the data\n",
        "        chunk = self.data[idx:idx + self.config.block_size + 1]\n",
        "        # encode every character to an integer\n",
        "        dix = [self.stoi[s] for s in chunk]\n",
        "        # return as tensors\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "\n",
        "def get_config():\n",
        "    C = CN()\n",
        "    C.system = CN()\n",
        "    C.system.seed = 3407\n",
        "    C.system.work_dir = './out/chargpt'\n",
        "    C.data = CharDataset.get_default_config()\n",
        "    C.model = GPT.get_default_config()\n",
        "    C.model.model_type = 'gpt-mini'\n",
        "    C.trainer = Trainer.get_default_config()\n",
        "    C.trainer.max_iters = 100\n",
        "    C.trainer.batch_size = 32\n",
        "    C.trainer.learning_rate = 5e-4\n",
        "    return C\n",
        "\n",
        "# configuration\n",
        "config = get_config()\n",
        "\n",
        "# Load tiny shakes data\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "text = open('input.txt', 'r').read()\n",
        "\n",
        "# Create training dataset\n",
        "train_dataset = CharDataset(config.data, text)\n",
        "\n",
        "# Create the model on tiny-shakes data\n",
        "config.model.vocab_size = train_dataset.get_vocab_size()\n",
        "config.model.block_size = train_dataset.get_block_size()\n",
        "model = GPT(config.model)\n",
        "\n",
        "# Create the Trainer\n",
        "trainer = Trainer(config.trainer, model, train_dataset)\n",
        "\n",
        "# Create the output directory\n",
        "output_dir = config.system.work_dir\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Train - Generate - Print - Repeat\n",
        "def gen(trainer):\n",
        "    if trainer.iter_num % 5 == 0:\n",
        "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            context = \"O God, O God!\"\n",
        "            x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None, ...].to(trainer.device)\n",
        "            y = model.generate(x, 500, temperature=1.0, do_sample=True, top_k=10)[0]\n",
        "            completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
        "            print(f'Generated Text after {trainer.iter_num} iterations:\\n{completion}\\n')\n",
        "        # save model\n",
        "        ckpt_path = os.path.join(output_dir, \"model.pt\")\n",
        "        torch.save(model.state_dict(), ckpt_path)\n",
        "        # Revert model to training\n",
        "        model.train()\n",
        "\n",
        "# Start gen\n",
        "trainer.back_end_callback('on_batch_end', gen)\n",
        "\n",
        "# Run the optimization\n",
        "trainer.run()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqjPUjwxTphq",
        "outputId": "bf2038dc-65bf-4bf3-d9dd-052ec6819eb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-04 01:53:50--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.6’\n",
            "\n",
            "input.txt.6         100%[===================>]   1.06M  5.15MB/s    in 0.2s    \n",
            "\n",
            "2023-12-04 01:53:51 (5.15 MB/s) - ‘input.txt.6’ saved [1115394/1115394]\n",
            "\n",
            "data has 1115394 characters, 65 unique.\n",
            "number of parameters: 2.71M\n",
            "running on device cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter_dt 0.00ms; iter 0: train loss 4.21075\n",
            "Generated Text after 0 iterations:\n",
            "O God, O God!udPdN  u gs hgere ss P   esNhPeP n s  t s Pneu meg  PsgNN t gu sunghsePee n tga s unne tgP n eeePrn t  te g heung sds w  g nsrn t  thgnras ht hragddeg hgta  sn tr tunrt eEtn mggdtasagrddEaatnrPng  arPsr hue s neee  nngs a tees n e mrggdng n nsaensg wetst nE er hens  nedtt tgPdn g srst t ss n sE a weearathugagasgs t  e g tgseens wtartreneE n anut t hgeg  wrsEnrrse t wPsenEg tsnnudenn enn henngunte trd  s aae saEst teede n wn h eags h e eersse h weg hetud nsrnd aar tgrdgsteede nrrsssn  ngnreEsde t\n",
            "\n",
            "saving model\n",
            "iter_dt 2417.68ms; iter 5: train loss 3.30040\n",
            "Generated Text after 5 iterations:\n",
            "O God, O God!\n",
            "\n",
            "N bion moth\n",
            "Sosse\n",
            "M ws\n",
            "\n",
            "\n",
            "han\n",
            "\n",
            "Mtrsaen t w wotoad\n",
            "N\n",
            "\n",
            "\n",
            "\n",
            "D\n",
            " b tAoreee anaato f wr\n",
            "\n",
            "C\n",
            "\n",
            "\n",
            "\n",
            "LAne w as hs alene\n",
            "M b m t\n",
            "Lhd,Ty ths,\n",
            "\n",
            "Ls hrs s yat hsalern t ae wad t\n",
            "\n",
            "Alon s wonoon msenor wd w boensithants t so se bn ond ty\n",
            "\n",
            "had orth s o mt\n",
            "\n",
            "\n",
            "Nt\n",
            "\n",
            "Ssh at\n",
            "Yar as wsard s b mee aes is onane ttate\n",
            "Md trrsieoaeedos alaot tynee w thd thindre s nnendaeeents\n",
            " wnrs y we\n",
            "he oanes\n",
            "Nr s n nota brs t\n",
            "\n",
            "\n",
            "\n",
            "Ths so s\n",
            "\n",
            "Aeneou as h thoooe nt maee nee t hir t wi mso wdst t t th t t\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "h aoed wene thenieedes\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Mte ats\n",
            "\n",
            "saving model\n",
            "iter_dt 2814.35ms; iter 10: train loss 3.03762\n",
            "Generated Text after 10 iterations:\n",
            "O God, O God!\n",
            "Les tto yre,ere beend s owe n s ted are hithe s,\n",
            "\n",
            "Ysher the t ord wa ndrd ad tsn madrott e tht hhonnennnete thatthe i sale thathas he,\n",
            "Y yee t aa hooua an tar hounortene herood he intin ir tene,'d y whathondo menimosr an s my ant thanthasil the tho ay iased we, t totour mie o arresse ands wed alond t t s be ido aner shen y isaatol ilar y bthoou is ou te morad asitotor te yitilth s maattthe y y thenalerierd id tannsod ouli tsd atrs youne arotatoue t anis ined\n",
            "Y wense yass hen hay an t y y so ane\n",
            "\n",
            "saving model\n",
            "iter_dt 2811.63ms; iter 15: train loss 2.85410\n",
            "Generated Text after 15 iterations:\n",
            "O God, O God! n fone bent t buthat t\n",
            "Angs, ons amand we broeler on lenes seaed orshesindas th adesre then solad s t\n",
            "Bn senelishtrer,\n",
            "\n",
            "\n",
            "She theraad,\n",
            "Thes s\n",
            "Ttherol wo nere s\n",
            "Fassamarousooeatan win ist otothorerere s id id moo thed ounsas\n",
            "\n",
            "Tholan aaril ta bt whlouss t\n",
            "\n",
            "\n",
            "Ar he ne t ngania s t ths nd s,\n",
            "S:\n",
            "That,\n",
            "G\n",
            "\n",
            "A orimre bestrareene and my, arimor h orar tht, as n me ninshenlerenos mer, t te t tame s thile thooo thiteriando thires mo wherd wo mare tilereaarones, somot an mo t ouetelor nnatrelime malore te w n\n",
            "\n",
            "saving model\n",
            "iter_dt 3138.61ms; iter 20: train loss 2.79577\n",
            "Generated Text after 20 iterations:\n",
            "O God, O God!\n",
            "The stou sher to hesstee t or t t otsthele wa indicoorowine, thad m that ourest,hithee saseate ous a that heale,roro terinicy hie tas tesengr,\n",
            "Thorite atonome lthineshatso watsrt be out o ou ingeninsishtte tilyhar areartte be t h sst t ilinounsou wher ithono tast our, w our te mine ther a wre on onseso torand o o in hshetofofollisad t tanee o ishe tinnouth hisdierestthieathe tof mens ou henotrerte mesiee mo ithat, t lse hant myofr, in outitin se tatr isen hericoust tin londsad bitow thee mangat\n",
            "\n",
            "saving model\n",
            "iter_dt 2885.42ms; iter 25: train loss 2.71072\n",
            "Generated Text after 25 iterations:\n",
            "O God, O God!\n",
            "ENEO:\n",
            "To wurllrsathen, handealld stth are, s b as f and\n",
            "EO indis t otis whisane wtt of t\n",
            "\n",
            "O: fusereithas hou bnd wis, myomeracou w\n",
            "EThin tindil sacaan ators fot werilsend whath ithathe mel sonesingtoualis irile anons he trs\n",
            "ARAnlathelsano ben helouse b hamar he in ade te, s agothaleate tonsome mouthtom,\n",
            "Andonerle thande thereateere idsarasis me s b t s,\n",
            "The tt,\n",
            "EReas wane wh t\n",
            "LAN:\n",
            "\n",
            "\n",
            "ENAn,rlyhons thie shith hesofe t withore ber heas mealloul itiealeer mathad ous mime thareasichyoourilowoouandes\n",
            "\n",
            "saving model\n",
            "iter_dt 2489.43ms; iter 30: train loss 2.66053\n",
            "Generated Text after 30 iterations:\n",
            "O God, O God! balisendil t,\n",
            "E mat, te teren it tarthasou s sanoud w blras,\n",
            "\n",
            "ELon t\n",
            "And bee thyowol brandelat se ouanee tor hetaterand btherin t t te me ot,\n",
            "Sher wow, in acit se teendess mistican tadean brar masene thterands mer a thhes fouer me wer tein ante t f talillldantholle wt ares asie acine seer tort to achinges itand t ile w fare t medtithat,\n",
            "Amyethsofanore ar the mle toutild alomof a s s wer whon thot ths itace, it te tasoum,\n",
            "I, honsor, s ig boud o iconstt,\n",
            "Anghe fomesttin t wearste boterd wee athyo\n",
            "\n",
            "saving model\n",
            "iter_dt 2255.68ms; iter 35: train loss 2.61948\n",
            "Generated Text after 35 iterations:\n",
            "O God, O God!\n",
            "WIORI yowads thinene f the,,\n",
            "\n",
            "Not f y h h bude ireassen, ad t me f or shis s t wan thelor, sooror b an thanise amal w athon mlitigooneacon m bad weder ha around the tane, and holllll a har womoullotis be aminor wome my f morerd mis angistonesor wh it\n",
            "S amed hillond tharet h,hatheenllo t acar, beasind t tt m t\n",
            "Thotas selin al ave athend ble ave s,\n",
            "\n",
            "Holldeetitoudeedsthel, alenousthad h wshy man bist tharerd t he ither me hower my,\n",
            "Wan mly amerenllert,ingeleld ass athe iminim fouderthas the me we \n",
            "\n",
            "saving model\n",
            "iter_dt 2267.31ms; iter 40: train loss 2.60455\n",
            "Generated Text after 40 iterations:\n",
            "O God, O God!s wes he totor thes h bitisithe, hendst tthariche mowanoushir, sss m hator, mouse othu th atrthin thilet talede tot cor me chavet tofime,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Burdenis,\n",
            "Tenotanit\n",
            "\n",
            "Had fe mat be,shinde o s alllle wileng acoumes cou we willllyoucantr myort m arart as an bre wan ser hantarend.\n",
            "\n",
            "Ar wird\n",
            "\n",
            "Wigrig cesind am thiced the myid t owendere to tive hearsth ir cllst beaco o selligle t hant o buse s bu wener berallde hinotout in marids al fofind waromar f celstt tasen h toowinouesthedththens,\n",
            "I tessetote cu w o\n",
            "\n",
            "saving model\n",
            "iter_dt 2243.95ms; iter 45: train loss 2.57407\n",
            "Generated Text after 45 iterations:\n",
            "O God, O God! wis ang acestore wowinot n ser heearorasthad in t, fo t,\n",
            "Anoth al thath in ithe blsoroursthe th be furerst s t hisowea s angh hol houths w, hths mathofealedinorind this at mse, ise budet s\n",
            "\n",
            "And chis n t s hedirtome nt houlet wint n ntound.\n",
            "I t ssteadrolyouses t t ige w wous thameriss s thad se hhave thid asar be certanore thantethand mangothead masse wh,\n",
            "Asousothan is hedee coris ad wet s, cer berertorsh be w t\n",
            "I'g mes aresit belithinoustor mand bans, stoma madim ch hereres wrs sh t, tt\n",
            "Anorin \n",
            "\n",
            "saving model\n",
            "iter_dt 2236.81ms; iter 50: train loss 2.52995\n",
            "Generated Text after 50 iterations:\n",
            "O God, O God! thar itase t thooo hind ninowoues wof where t t wouleir hangre tread mly mouse t a bind ma thoreimotthaso men mricth t ans f t m ant, nome to thuind,\n",
            "Nothe f atheth thout wee thy ithe wanse t ildind h s t hid war whe thintharino s alld methand wat s st tord tad, t aisoum wisthanes, ssthe whit heroushothe w th stor herd as anghame n amtorsest be fat ford n be th wamur bour nthe we bund th s ninin n thathend w buithithorortouthenortisthidors, menger bur morsss. he s blo mour fos mig s we mer ire \n",
            "\n",
            "saving model\n",
            "iter_dt 2829.36ms; iter 55: train loss 2.55926\n",
            "Generated Text after 55 iterations:\n",
            "O God, O God!hil he thy bre, have,\n",
            "A:\n",
            "The fres wethowe seer thof arourof s tav womatar my,\n",
            "Bo th ilat myothe th handristhe the mast tene theerr,\n",
            "Wothin bl bim m w tofataval aso the s iseler tithin she o foro f treasthencat houst tourd benisst is thard the t st stistt s he tinterarer m ad orthed har o m be ser adr or ben tonteertit tenclisthent ttomed s s s merarthatheresish mede frertenof ort s f ithate he,\n",
            "Ases had.\n",
            "Ca me othe bas sioonger s, s.\n",
            "Anon ou omy s had h, ted we withedrerd ma thade muto won wend \n",
            "\n",
            "saving model\n",
            "iter_dt 3062.79ms; iter 60: train loss 2.59380\n",
            "Generated Text after 60 iterations:\n",
            "O God, O God!\n",
            "Whod adonst ard.\n",
            "T s tharerartuntom the,\n",
            "\n",
            "Wheet isthe an, boreshe whe,\n",
            "\n",
            "Wee t t tothalay t wirat inoon, ay tu w,\n",
            "AS:\n",
            "I weanind of brealidrool bes a thistinstho othe hisilave ton w w f ter wathe homy f hidor me he id\n",
            "\n",
            "Theerorout bre,\n",
            "IO:\n",
            "ASis my therthatin fr or sh fofal s\n",
            "T:\n",
            "Wade ay be werealarathishand hin, sithorert t\n",
            "Ma sthell, wan ban, ou s s hily,he h t her hed herd arane t hay timimit ot wee sth areded ande s aly,\n",
            "IOLarsarerdavetas wieleated\n",
            "Thorathin fas damis,\n",
            "Tett s sthis, hincarethare\n",
            "\n",
            "saving model\n",
            "iter_dt 2880.56ms; iter 65: train loss 2.53317\n",
            "Generated Text after 65 iterations:\n",
            "O God, O God! sis herthea ssthengr mon m brth ton fout hatou se t fourd aran stheretheass ns t tth th oueshame wng ad bese se sirinonds,\n",
            "\n",
            "Soun, tharatanous beathesonigan was t an f s,\n",
            "Angillouseng t fal t whans s al wesearaneare hiore sind m t whond ich m hotis s ansit h myoul fee, in wed ale hortan ind,\n",
            "Wanethalave hor ad wil theallld out ille se sshsthes f wig iseree worelltherthee site aver thy sthoshisears men te batha so thir m st s orthe the tin s ware tuimestr mant ithiceathon se meathe t he, whoun ow\n",
            "\n",
            "saving model\n",
            "iter_dt 2722.53ms; iter 70: train loss 2.54164\n",
            "Generated Text after 70 iterations:\n",
            "O God, O God!\n",
            "TIOL:\n",
            "Burof to his t blatigon se wesear st, be wenth fu, in watithe blar, wichese aris th bo t alard sthalerd ther say myealy t tathe fitene o h fothand our ad w hathrtho t s hansthe tofe iller o w wshe weaveathhou sellor thens\n",
            "\n",
            "\n",
            "\n",
            "S:\n",
            "\n",
            "CEURUMartheris tayat t t, wh m athand the tierede t, merily a out owhe, s tithitathas thithe m s illl this bin bate f theavilanerde he t t oue st t bly,\n",
            "An the t, sio is beance shrarese, wareas we ire wimthisind t ferirshoul my hesh idardieat f m im hot.\n",
            "\n",
            "ALI'cour\n",
            "\n",
            "saving model\n",
            "iter_dt 2268.42ms; iter 75: train loss 2.50449\n",
            "Generated Text after 75 iterations:\n",
            "O God, O God!\n",
            "I son halsaled, meert d frd,\n",
            "Andr thicr wimirot h maran h, s, t fesour ncllasutre me the bume hore thid\n",
            "TAn shiceshamies anthy n he in athant sowins hethes anced tot ad buse hr my t makerom w, are shesss thurors we aner thave by maneler o fon thed owhus, hy,\n",
            "\n",
            "Thimulowand w, hy souncus tonthers band s sotho s he by hin noun thy nthisilshe my, my tr whe thie wiler the whe,\n",
            "Ifol blay than hy ot mas ad me mard anot d ble, w, o bu thinorumy an, my frs offithist te ncand tho sthy fotut,\n",
            "Anchon al her\n",
            "\n",
            "saving model\n",
            "iter_dt 2236.69ms; iter 80: train loss 2.52064\n",
            "Generated Text after 80 iterations:\n",
            "O God, O God!\n",
            "MIS:\n",
            "Sicrer bee theaved fan tissind borat ar thild, were ote t be tinore is\n",
            "Honge he bame sound hin bel or wans is wsthes wiss otit athon m. thereng inkothis moumar t omess m t thethithede bede in antheatareralens on went ar,\n",
            "The thads bores anda m womotillande t wimes this thanoutas be h t the wane isearind, he ave s, th wirat ine m mee wino it sis med,\n",
            "Is ithongruthe ase med ther the s beanthe, s t theankave this thin far inde the sheno t adathime brond ineld,\n",
            "Ale touno mearengicuse, he my to\n",
            "\n",
            "saving model\n",
            "iter_dt 2251.55ms; iter 85: train loss 2.50730\n",
            "Generated Text after 85 iterations:\n",
            "O God, O God!\n",
            "MASIONou IO thatho a anon atrorshart d atheass foud hedoulins, t then icue forse win timindelom\n",
            "Core omear, alllour se owosee, t we ar ase f isasice torirth aimire athe w wita is have and, oucicr, t inth wind but wit an,\n",
            "Wofou houreleareroulimy an ourorest ban trthitiour o wito the s in the at,\n",
            "Andasathe theratheseeall thas of h blow hing thes ar, w thowh s thant,\n",
            "Woorentingeasof t ber t or hon ancotheayshe felin m be illere, t be akenger mulloth t sh f outheist\n",
            "Wath be teanishe mestheave t,\n",
            "As\n",
            "\n",
            "saving model\n",
            "iter_dt 3145.26ms; iter 90: train loss 2.51205\n",
            "Generated Text after 90 iterations:\n",
            "O God, O God!\n",
            "Tow dase wal dingosthy sell hid woure s mard toucis my sent the tho hes where thad bitondin tore s tsed word sowigor fea wonco an, home hen oomy hits wis an thenosha f mino n be tinerulisee brs astre\n",
            "Asthades hort t wot wes f terare f fe teenshas alare anthinchy ofuntere at t, at cese b fid t sthit houre wrasilend,\n",
            "Win mere mos nor allimund be the ar ho cengea wounincom filome ngin tout t cthe tathe thace theto che find anthingear tosithen tare\n",
            "\n",
            "She whe hs cofin fous, at havethe mpreathon s s s\n",
            "\n",
            "saving model\n",
            "iter_dt 2270.91ms; iter 95: train loss 2.48614\n",
            "Generated Text after 95 iterations:\n",
            "O God, O God!\n",
            "Whed wesuth tant by fo athot f wot te,\n",
            "An aveat oulisthe, hant f fote tonor mye tha by hadof m se,\n",
            "\n",
            "\n",
            "I t t marerofoulal t thofio hid avat stimithis hyot micis ayo we br, we oriothe th my averingar thins but t therenounde h than,\n",
            "\n",
            "Ate weste thay het fares.\n",
            "Angas t fous wne met besichy wherass as f mathit heee s whes mered akes myon t, t beerele hine the and fo fie t wered t t oonde fulenom t athee\n",
            "ARI te teead chin my\n",
            "Tho ance spa wod\n",
            "S:\n",
            "Sod fall athase she t thangheloug, m th homowatho hour f c\n",
            "\n",
            "saving model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from mingpt.model import GPT\n",
        "from mingpt.trainer import Trainer\n",
        "from mingpt.utils import set_seed, setup_logging, CfgNode as CN\n",
        "\n",
        "#get info about the shakes dataset\n",
        "class CharDataset(Dataset):\n",
        "    @staticmethod\n",
        "    def get_default_config():\n",
        "        C = CN()\n",
        "        C.block_size = 128\n",
        "        return C\n",
        "\n",
        "    def __init__(self, config, data):\n",
        "        self.config = config\n",
        "\n",
        "        chars = sorted(list(set(data)))\n",
        "        data_size, vocab_size = len(data), len(chars)\n",
        "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "\n",
        "        self.stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "        self.itos = {i: ch for i, ch in enumerate(chars)}\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = data\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return self.vocab_size\n",
        "\n",
        "    def get_block_size(self):\n",
        "        return self.config.block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.config.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab a chunk of (block_size + 1) characters from the data\n",
        "        chunk = self.data[idx:idx + self.config.block_size + 1]\n",
        "        # encode every character to an integer\n",
        "        dix = [self.stoi[s] for s in chunk]\n",
        "        # return as tensors\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "\n",
        "def get_config():\n",
        "    C = CN()\n",
        "    C.system = CN()\n",
        "    C.system.seed = 3407\n",
        "    C.system.work_dir = './out/chargpt'\n",
        "    C.data = CharDataset.get_default_config()\n",
        "    C.model = GPT.get_default_config()\n",
        "    C.model.model_type = 'gpt-mini'\n",
        "    C.trainer = Trainer.get_default_config()\n",
        "    C.trainer.max_iters = 1000\n",
        "    C.trainer.batch_size = 32\n",
        "    C.trainer.learning_rate = 5e-4\n",
        "    return C\n",
        "\n",
        "# configuration\n",
        "config = get_config()\n",
        "\n",
        "# Load tiny shakes data\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "text = open('input.txt', 'r').read()\n",
        "\n",
        "# Create training dataset\n",
        "train_dataset = CharDataset(config.data, text)\n",
        "\n",
        "# Create the model on tiny-shakes data\n",
        "config.model.vocab_size = train_dataset.get_vocab_size()\n",
        "config.model.block_size = train_dataset.get_block_size()\n",
        "model = GPT(config.model)\n",
        "\n",
        "# Create the Trainer\n",
        "trainer = Trainer(config.trainer, model, train_dataset)\n",
        "\n",
        "# Create the output directory\n",
        "output_dir = config.system.work_dir\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Train - Generate - Print - Repeat\n",
        "def gen(trainer):\n",
        "    if trainer.iter_num % 100 == 0:\n",
        "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            context = \"O God, O God!\"\n",
        "            x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None, ...].to(trainer.device)\n",
        "            y = model.generate(x, 500, temperature=1.0, do_sample=True, top_k=10)[0]\n",
        "            completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
        "            print(f'Generated Text after {trainer.iter_num} iterations:\\n{completion}\\n')\n",
        "        # save model\n",
        "        ckpt_path = os.path.join(output_dir, \"model.pt\")\n",
        "        torch.save(model.state_dict(), ckpt_path)\n",
        "        # Revert model to training\n",
        "        model.train()\n",
        "\n",
        "# Start gen\n",
        "trainer.set_callback('on_batch_end', gen)\n",
        "\n",
        "# Run the optimization\n",
        "trainer.run()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdFLiSPrk_C4",
        "outputId": "ca1eac2d-2d11-482e-c114-e7a83420bbfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-04 02:09:56--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.9’\n",
            "\n",
            "input.txt.9         100%[===================>]   1.06M  5.17MB/s    in 0.2s    \n",
            "\n",
            "2023-12-04 02:09:56 (5.17 MB/s) - ‘input.txt.9’ saved [1115394/1115394]\n",
            "\n",
            "data has 1115394 characters, 65 unique.\n",
            "number of parameters: 2.71M\n",
            "running on device cpu\n",
            "iter_dt 0.00ms; iter 0: train loss 4.23563\n",
            "Generated Text after 0 iterations:\n",
            "O God, O God!n&' me 'uie m' ru ir' eQl emi n'tieQnt uuQrmQMu   mntie Ql  'u' mnor rQr  e  e''   i irmi lQno te  re e oe, e,iiruir''oe mi\n",
            "o enuenr 'uunhanee m '  heir 'ii'reee'eru  i o m'n  rnterurr'i  'ue re 'e 'e u'rnu th o''uu' m'ee$ ha m hSentS'eenuoe t  ' u h o r'  haerniesi'e m'ue'ilnl inurin iri ent  hathoi' tesr$r ou eshu ummere, en ' eires i tMu  rnot meso h 'tSM'im'u o hhin ruoi onrimirur 't rieire'in eri\n",
            "rnurouoiinoh\n",
            "ernhhu'hi 'ioe tSi ril'rmen ro ue  rh\n",
            "hel'enroliot u t 'oriesh\n",
            " ilC t  hurrhie on \n",
            "\n",
            "iter_dt 2243.87ms; iter 100: train loss 2.46459\n",
            "Generated Text after 100 iterations:\n",
            "O God, O God!\n",
            "WA:\n",
            "A:\n",
            "Wine so t amatrerer ainerou ise s d be ies funadarear, t, a myo th mante t,\n",
            "ANofort dotre s he foousalstancth fouer amyeth boout t, ot tte ossor,\n",
            "Ho thethed wh man weroowe the t in hene byon thout h anthineasenore he bane to sure at oofo wise t wnd mo ateror hor ay f f he, he he thallothithanoote s br sth fe hifth womy,\n",
            "TI toore ild toutind aren blle he ores, mos s o ss meatimy isthowe,\n",
            "CI m malen ane hetare fo s,\n",
            "ARTI ber outa te oure se alemy far me s fandinoungime our biting s her s h\n",
            "\n",
            "iter_dt 2354.09ms; iter 200: train loss 2.39170\n",
            "Generated Text after 200 iterations:\n",
            "O God, O God!\n",
            "BISE:\n",
            "Anorthst he y sthind be byor doulong sue dend ththe meshers wincindis, ansthis it t bed\n",
            "Win ardon thed ilares bla angssedes sthellino a ars medsheatho the be the athean teanont ato athomed to thar illllldsedas ind\n",
            "The trere, anofinomenday min aves bun al angrand as, thaneallinenoth, he hend.\n",
            "\n",
            "\n",
            "Fa t allasp ayorelfowatlest it sarersa she that brn a male wino marerangrd orin ases ashay the besatr fucke, sis tond oucimind\n",
            "The h.\n",
            "Biromer mat s atis aren INE:\n",
            "Ansharand sous he wnesinghaver bo b\n",
            "\n",
            "iter_dt 2390.96ms; iter 300: train loss 2.39123\n",
            "Generated Text after 300 iterations:\n",
            "O God, O God!\n",
            "\n",
            "Harurit stet yore asirsou thof thy ton tho thele thir thyee, and\n",
            "Foue t it surspiss ay t meatinghshe,\n",
            "For s timy illan: billdo h hine hind hin, ichead hoou ay, t sure wotime.\n",
            "\n",
            "\n",
            "Ther, ICKILUCEDAUS:\n",
            "WARE:\n",
            "Hald mo har hearithort bere, mure iss.\n",
            "\n",
            "\n",
            "Macure ISThand he yengertay t arstho ivee y and ars\n",
            "I spray siseearting. th ff trears thion wore ace\n",
            "\n",
            "Aur the wh hait ticeshind t fat in indirsesis,\n",
            "Hres\n",
            "Pu ontellll, ourar be mara bul, mouch onor.\n",
            "Wash he tiealy f beay, mice s it ho willarte.\n",
            "PAnge bed \n",
            "\n",
            "iter_dt 2239.88ms; iter 400: train loss 2.24801\n",
            "Generated Text after 400 iterations:\n",
            "O God, O God!\n",
            "TRIIOO:\n",
            "And murd whesthe thy her ice tou shourdim?\n",
            "\n",
            "LOUMENCERO:\n",
            "O meer inces tooughthsttht wim illere there.\n",
            "\n",
            "Ses meaththerser yeat hou irtse shee iferese.\n",
            "\n",
            "\n",
            "LOUS:\n",
            "Hans wise hanor tust you is tole,\n",
            "Hayers, wer youll me of her sourd maystede\n",
            "I heelld mad amard our ond wank.\n",
            "\n",
            "\n",
            "WAKARULIIE:\n",
            "Anth, the theay thour our thr tomy.\n",
            "\n",
            "Thers whe hell thil isheliou mbus ba dicthint,\n",
            "The the ar al ome ane shee has mil heer as mes meam.\n",
            "\n",
            "WALLION:\n",
            "Thay, mu atiss y.\n",
            "\n",
            "\n",
            "\n",
            "LUCEUNEON:\n",
            "\n",
            "Cowend weray wal ou mear.\n",
            "\n",
            "\n",
            "COR\n",
            "\n",
            "iter_dt 3617.86ms; iter 500: train loss 2.15924\n",
            "Generated Text after 500 iterations:\n",
            "O God, O God!\n",
            "\n",
            "BULENCES:\n",
            "I hat spruccke thar, she to have me ant; and,\n",
            "And therig the he my marion wor hy soth\n",
            "Thyse say my fa wicedor hin thy breity a mars oowald.\n",
            " ma tharr momor tof mour the the whey fir is myor a tof wutar suenee deear,\n",
            "Apate bleivor hish sho bith tin of ine a\n",
            "Ifall ato thr tof wa whay be seyor our his warthe ins ore han of,\n",
            "Ha don sha hear wint bacoug ho otr mento\n",
            "Bur this stheir hatom shinns munds shell fay mort ind a beat me han,\n",
            "Whand oorl dof blaverin bet hompes.\n",
            "\n",
            "AKDWAPED:\n",
            "Wher bee\n",
            "\n",
            "iter_dt 3241.74ms; iter 600: train loss 2.05064\n",
            "Generated Text after 600 iterations:\n",
            "O God, O God!\n",
            "ARCEOP:\n",
            "I be are wicked and mesteane of frecr he theere mirsin\n",
            "to ard fith in sum, the the mat that of troncins:\n",
            "Thi hit on thou for hill to waly a a he an.\n",
            "\n",
            "PROM:\n",
            "Why, my you her it that as and be ttrume,\n",
            "And witthe to betoed to of wor, if oure a for,\n",
            "Whe ast, the to tigh's, soucld to wild I wa tant\n",
            "I seair, the meace thy of at tat, tour thue\n",
            "To ir mill of with thee day ar bethigh,\n",
            "Suls thist but shoull sto it, sa that of treate\n",
            "Bet tike ot' and bie ing,\n",
            "And bot tay on stay file airsent,\n",
            "And w\n",
            "\n",
            "iter_dt 2326.14ms; iter 700: train loss 1.97737\n",
            "Generated Text after 700 iterations:\n",
            "O God, O God!\n",
            "\n",
            "MENRUS:\n",
            "The doming shild agidy swiel may will,\n",
            "To wheer wo tein ap if me of it westh\n",
            "Anet which pe theernt, brainnt whath, is math\n",
            "Ineing humeffen to hat thy donk innes the tobly oners\n",
            "Have wall.\n",
            "\n",
            "MEORNCENE:\n",
            "Het shaper's mishing mence. \n",
            "\n",
            "BRASCOLO:\n",
            "As hame what nies and briend and it.\n",
            "\n",
            "BRIICY:\n",
            "Therrves wereall bing ther wult, anet.\n",
            "\n",
            "SRUTICENE:\n",
            "And stim arke whiths he sad wast bland,\n",
            "Of yORD IIV:\n",
            "Whe comst, in in our hown hamers a tuth that and.\n",
            "\n",
            "PUMIO:\n",
            "And o't mer it sin of low, bot,\n",
            "We ande as\n",
            "\n",
            "iter_dt 3097.69ms; iter 800: train loss 1.90950\n",
            "Generated Text after 800 iterations:\n",
            "O God, O God!\n",
            "\n",
            "ANARILANE:\n",
            "I my my dantard ther of you, for will,\n",
            "He dencoun all we wild sto he pacuons, thou with this hand.\n",
            "\n",
            "MENRY VOBROLIARENG:\n",
            "O conce, and me, I stome burseand, I and aurs thenstser:\n",
            "And but in trainks i on tuo oure thand ame\n",
            "As id hear best of shanng one bing of shall ans as.\n",
            "\n",
            "CAUFCET:\n",
            "Wicome, me hights, I heart a withe by deace.\n",
            "\n",
            "COUCETLOO:\n",
            "Sicions,\n",
            "And thes of will that fars thou be hear,\n",
            "Heer saless, so bide heare thanks me comess.\n",
            "\n",
            "KING MENVIO:\n",
            "I that be cust should, shall at soffeet\n",
            "\n",
            "iter_dt 2556.60ms; iter 900: train loss 1.88981\n",
            "Generated Text after 900 iterations:\n",
            "O God, O God!\n",
            "\n",
            "KINGHARIIND:\n",
            "I what's with as heave it as sing me is my ham.\n",
            "\n",
            "LUCH:\n",
            "Nows hart the will at of the faters, his as\n",
            "I leaths ot ments and as flaciut sast,\n",
            "This himshiness theight to of worded think,\n",
            "Hyss it make then sin to fon wast sham to cont.\n",
            "\n",
            "GROM:\n",
            "Mothed your for myightsss ast it thou mun.\n",
            "\n",
            "CLUCENTER:\n",
            "Well you to this other the wile of will to mare\n",
            "This brot of thonou sply for tattes these sacl,\n",
            "As of of was is, thou sust ot to toor him.\n",
            "Thalt too man: a hart him!\n",
            "Host with me the see of to \n",
            "\n"
          ]
        }
      ]
    }
  ]
}